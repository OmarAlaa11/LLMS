{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG54RRFGXDI9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sacrebleu torch sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Load Pre-trained Model & Dataset"
      ],
      "metadata": {
        "id": "qfMGJHQvXPpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the dataset (e.g., FLORES-200)\n",
        "dataset = load_dataset(\"facebook/flores\", \"eng_Latn-arb_Arab\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Nogab_dLXEB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the Data"
      ],
      "metadata": {
        "id": "Kjbzu6kQXL0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(examples):\n",
        "    inputs = tokenizer(examples[\"sentence_eng\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    targets = tokenizer(examples[\"sentence_arb\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "val_dataset = tokenized_dataset[\"validation\"]\n"
      ],
      "metadata": {
        "id": "vGufuN5aXEEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning the Model"
      ],
      "metadata": {
        "id": "uWZzmSyJXTcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./bert_translation\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "n-6p5dGJXEG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Model"
      ],
      "metadata": {
        "id": "UU371oeKXZaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Function to translate a sentence\n",
        "def translate(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Example Evaluation\n",
        "references = [\"مرحبًا بك في العالم\"]\n",
        "predictions = [translate(\"Welcome to the world\")]\n",
        "bleu_score = corpus_bleu(predictions, [references])\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score.score}\")\n"
      ],
      "metadata": {
        "id": "7_ri3tpJXEJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./bert_translation_finetuned\")\n",
        "tokenizer.save_pretrained(\"./bert_translation_finetuned\")\n",
        "\n",
        "# Load Later\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "model = MarianMTModel.from_pretrained(\"./bert_translation_finetuned\")\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"./bert_translation_finetuned\")\n"
      ],
      "metadata": {
        "id": "aM7o0svtXELd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2bwGqIuXEU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}